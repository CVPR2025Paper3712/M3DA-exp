import numpy as np
import torch
import torch.nn as nn
from dpipe.predict import add_extract_dims, divisible_shape
from dpipe.predict.shape import patches_grid
from dpipe.torch import inference_step
from dpipe.train.policy import Switch
from nnunetv2.training.loss.compound_losses import DC_and_CE_loss
from nnunetv2.training.loss.deep_supervision import DeepSupervisionWrapper
from nnunetv2.training.loss.dice import MemoryEfficientSoftDiceLoss
from torch.nn import functional as F

from m3da_exp.batch_iter import SPATIAL_DIMS
from m3da_exp.torch.functional import softmax
from m3da_exp.torch.model import train_step_se as train_step  # !!! replacing original train step !!!
from m3da_exp.torch.module.nnunet import PlainConvUNet, DeepSupervisionTargetWrapper
from m3da_exp.torch.utils import nnunet_lr_schedule_fn


# # model
return_features_from = -1

x_patch_size = y_patch_size = np.array([160, 160, 64])
batch_size = 2

# pool_op_kernel_sizes = [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]]  # (nnUnet auto)
pool_op_kernel_sizes = [[1, 1, 1], [2, 2, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]]  # (orig orientation)
conv_kernel_sizes = [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]
num_stages = len(conv_kernel_sizes)
base_num_features = 24  # 32
max_num_features = 320
features_per_stage = [min(base_num_features * 2 ** i, max_num_features) for i in range(num_stages)]
n_conv_per_stage_encoder = [2, 2, 2, 2, 2, 2]
n_conv_per_stage_decoder = [2, 2, 2, 2, 2]
# norm_op = nn.InstanceNorm3d
norm_op = nn.BatchNorm3d
deep_supervision = True
init_bias = None  # it does not help to converge on this dataset anyway
arch_kwargs = dict(input_channels=n_chans_in, n_stages=num_stages, features_per_stage=features_per_stage,
                   conv_op=nn.Conv3d, kernel_sizes=conv_kernel_sizes, strides=pool_op_kernel_sizes,
                   n_conv_per_stage=n_conv_per_stage_encoder, num_classes=n_chans_out,
                   n_conv_per_stage_decoder=n_conv_per_stage_decoder, conv_bias=True, norm_op=norm_op,
                   norm_op_kwargs={'eps': 1e-5, 'affine': True}, dropout_op=None, dropout_op_kwargs=None,
                   nonlin=nn.LeakyReLU, nonlin_kwargs={'inplace': True}, deep_supervision=deep_supervision,
                   init_bias=init_bias, return_features_from=return_features_from)
student = PlainConvUNet(**arch_kwargs)
teacher = PlainConvUNet(**arch_kwargs)
architecture = nn.ModuleList([student, teacher])


# # loss
batch_dice = True
loss = DC_and_CE_loss({'batch_dice': batch_dice, 'smooth': 1e-5, 'do_bg': False, 'ddp': False}, {},
                      dice_class=MemoryEfficientSoftDiceLoss)
deep_supervision_scales = list(list(i) for i in 1 / np.cumprod(np.vstack(pool_op_kernel_sizes), axis=0))[:-1]
_supervision_weights = np.array([1 / (2 ** i) for i in range(len(deep_supervision_scales) - 1)] + [0])
# _supervision_weights[-1] = 0
supervision_weights = _supervision_weights / _supervision_weights.sum()
criterion_task = DeepSupervisionTargetWrapper(DeepSupervisionWrapper(loss, supervision_weights),
                                              pool_op_kernel_sizes, labels_list, device)
# criterion_cons = DeepSupervisionWrapper(F.mse_loss, supervision_weights)
criterion_cons = F.mse_loss
criterion = {'segm': criterion_task, 'cons': criterion_cons}

# # optimizer
batches_per_epoch = 250
n_epochs = 600
lr_init = 1e-2
lr = Switch(lr_init, {i: nnunet_lr_schedule_fn(i, lr_init, n_epochs=n_epochs) for i in range(n_epochs)})
# weight_decay = 3e-5
clip_grad = 12

weight_decay = 1e-8
# optimizer = torch.optim.SGD(student.parameters(), lr=lr_init, momentum=0.99, nesterov=True, weight_decay=weight_decay)
optimizer = torch.optim.AdamW(student.parameters(), lr=lr_init, weight_decay=weight_decay)


# # train step
gamma = 2.0  # from https://arxiv.org/pdf/1811.06042.pdf
p_augm = 0.9
ema_alpha = 0.3  # teacher_{t + 1} = teacher_{t} * (1 - ema_alpha) * student_{t + 1} * ema_alpha
activation = softmax
train_kwargs = dict(lr=lr, student=student, teacher=teacher, ema_alpha=ema_alpha, gamma=gamma,
                    criterion_task=criterion_task, criterion_cons=criterion_cons, p=p_augm, optimizer=optimizer,
                    random_state=random_state, activation=activation)
checkpoints = Checkpoints(checkpoints_path,
                          {**{k: v for k, v in train_kwargs.items() if isinstance(v, Policy)},
                           'student.pth': student, 'teacher.pth': teacher, 'optimizer.pth': optimizer})


# # predict
pred_patch_size = x_patch_size
pred_patch_stride = pred_patch_size // 2


@add_extract_dims(n_add=pred_n_add_dims, n_extract=pred_n_extract_dims)
@patches_grid(pred_patch_size, pred_patch_stride, axis=SPATIAL_DIMS)
@divisible_shape(divisor=np.prod(pool_op_kernel_sizes, axis=0), padding_values=np.min, axis=SPATIAL_DIMS)
def predict(image):
    teacher.set_deep_supervision(False)
    pred = inference_step(image, architecture=teacher, activation=activation, amp=amp)
    teacher.set_deep_supervision(True)
    return pred


@add_extract_dims(n_add=pred_n_add_dims, n_extract=pred_n_extract_dims)
@patches_grid(pred_patch_size, pred_patch_size, axis=SPATIAL_DIMS)
@divisible_shape(divisor=np.prod(pool_op_kernel_sizes, axis=0), padding_values=np.min, axis=SPATIAL_DIMS)
def val_predict(image):
    teacher.set_deep_supervision(False)
    pred = inference_step(image, architecture=teacher, activation=activation, amp=amp)
    teacher.set_deep_supervision(True)
    return pred
