import numpy as np
import torch
import torch.nn as nn
from dpipe.predict import add_extract_dims, divisible_shape
from dpipe.predict.shape import patches_grid
from dpipe.torch import inference_step, weighted_cross_entropy_with_logits
from dpipe.train.policy import Switch
from nnunetv2.training.loss.compound_losses import DC_and_CE_loss
from nnunetv2.training.loss.deep_supervision import DeepSupervisionWrapper
from nnunetv2.training.loss.dice import MemoryEfficientSoftDiceLoss

from m3da_exp.batch_iter import SPATIAL_DIMS
from m3da_exp.torch.functional import softmax
from m3da_exp.torch.model import train_step_dann as train_step  # !!! replacing original train step !!!
from m3da_exp.torch.module.dann import Discriminator, DANN, DANNLoss
from m3da_exp.torch.module.nnunet import PlainConvUNet, DeepSupervisionTargetWrapper
from m3da_exp.torch.utils import nnunet_lr_schedule_fn


# # model
return_features_from = -1

x_patch_size = y_patch_size = np.array([160, 160, 64])
batch_size = 2

# pool_op_kernel_sizes = [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]]  # (nnUnet auto)
pool_op_kernel_sizes = [[1, 1, 1], [2, 2, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]]  # (orig orientation)
conv_kernel_sizes = [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]
num_stages = len(conv_kernel_sizes)
base_num_features = 24  # 32
max_num_features = 320
features_per_stage = [min(base_num_features * 2 ** i, max_num_features) for i in range(num_stages)]
n_conv_per_stage_encoder = [2, 2, 2, 2, 2, 2]
n_conv_per_stage_decoder = [2, 2, 2, 2, 2]
norm_op = nn.InstanceNorm3d
# norm_op = nn.BatchNorm3d
deep_supervision = True
init_bias = None  # it does not help to converge on this dataset anyway
arch_segm = PlainConvUNet(input_channels=n_chans_in, n_stages=num_stages, features_per_stage=features_per_stage,
                          conv_op=nn.Conv3d, kernel_sizes=conv_kernel_sizes, strides=pool_op_kernel_sizes,
                          n_conv_per_stage=n_conv_per_stage_encoder, num_classes=n_chans_out,
                          n_conv_per_stage_decoder=n_conv_per_stage_decoder, conv_bias=True, norm_op=norm_op,
                          norm_op_kwargs={'eps': 1e-5, 'affine': True}, dropout_op=None, dropout_op_kwargs=None,
                          nonlin=nn.LeakyReLU, nonlin_kwargs={'inplace': True}, deep_supervision=deep_supervision,
                          init_bias=init_bias, return_features_from=return_features_from)

n_disc_features = 256
arch_disc = Discriminator(in_channels=features_per_stage[return_features_from], n_features=n_disc_features)
architecture = DANN(segmentator=arch_segm, discriminator=arch_disc)


# # loss
batch_dice = True
loss = DC_and_CE_loss({'batch_dice': batch_dice, 'smooth': 1e-5, 'do_bg': False, 'ddp': False}, {},
                      dice_class=MemoryEfficientSoftDiceLoss)
deep_supervision_scales = list(list(i) for i in 1 / np.cumprod(np.vstack(pool_op_kernel_sizes), axis=0))[:-1]
_supervision_weights = np.array([1 / (2 ** i) for i in range(len(deep_supervision_scales) - 1)] + [0])
# _supervision_weights[-1] = 0
supervision_weights = _supervision_weights / _supervision_weights.sum()
criterion_segm = DeepSupervisionTargetWrapper(DeepSupervisionWrapper(loss, supervision_weights),
                                              pool_op_kernel_sizes, labels_list, device)
dann_alpha = 1e-2
criterion = DANNLoss(segmentation_criterion=criterion_segm,
                     discriminator_criterion=weighted_cross_entropy_with_logits, alpha=dann_alpha)


# # optimizer
batches_per_epoch = 250
n_epochs = 500
lr_init = 1e-2
lr = Switch(lr_init, {i: nnunet_lr_schedule_fn(i, lr_init, n_epochs=n_epochs) for i in range(n_epochs)})
weight_decay = 3e-5
clip_grad = 12

optimizer_segm = torch.optim.SGD(architecture.segmentator.parameters(),
                                 momentum=0.99, nesterov=True, lr=lr_init, weight_decay=weight_decay)
optimizer_disc = torch.optim.SGD(architecture.discriminator.parameters(),
                                 momentum=0.99, nesterov=True, lr=lr_init, weight_decay=weight_decay)


# # train step
train_kwargs = dict(lr=lr, architecture=architecture, optimizer_segm=optimizer_segm, optimizer_disc=optimizer_disc,
                    loss_key_segm="adversarial_loss", loss_key_disc="discriminator_loss", criterion=criterion)
checkpoints = Checkpoints(checkpoints_path, {
    **{k: v for k, v in train_kwargs.items() if isinstance(v, Policy)},
    saved_model_path: architecture, "optimizer_segm.pth": optimizer_segm, "optimizer_disc.pth": optimizer_disc
})


# # predict
pred_patch_size = x_patch_size
pred_patch_stride = pred_patch_size // 2


@add_extract_dims(n_add=pred_n_add_dims, n_extract=pred_n_extract_dims)
@patches_grid(pred_patch_size, pred_patch_stride, axis=SPATIAL_DIMS)
@divisible_shape(divisor=np.prod(pool_op_kernel_sizes, axis=0), padding_values=np.min, axis=SPATIAL_DIMS)
def predict(image):
    arch_segm.set_deep_supervision(False)
    pred = inference_step(image, architecture=arch_segm, activation=softmax, amp=amp)
    arch_segm.set_deep_supervision(True)
    return pred


@add_extract_dims(n_add=pred_n_add_dims, n_extract=pred_n_extract_dims)
@patches_grid(pred_patch_size, pred_patch_size, axis=SPATIAL_DIMS)
@divisible_shape(divisor=np.prod(pool_op_kernel_sizes, axis=0), padding_values=np.min, axis=SPATIAL_DIMS)
def val_predict(image):
    arch_segm.set_deep_supervision(False)
    pred = inference_step(image, architecture=arch_segm, activation=softmax, amp=amp)
    arch_segm.set_deep_supervision(True)
    return pred
