import numpy as np
import torch
import torch.nn as nn
from dpipe.predict import add_extract_dims, divisible_shape
from dpipe.predict.shape import patches_grid
from dpipe.torch import inference_step
from dpipe.train.policy import Switch
from nnunetv2.training.loss.compound_losses import DC_and_CE_loss
from nnunetv2.training.loss.deep_supervision import DeepSupervisionWrapper
from nnunetv2.training.loss.dice import MemoryEfficientSoftDiceLoss

from m3da_exp.batch_iter import SPATIAL_DIMS
from m3da_exp.torch.functional import softmax
from m3da_exp.torch.model import train_step_minent as train_step  # !!! replacing original train step !!!
from m3da_exp.torch.module.entropy import EntropyLoss
from m3da_exp.torch.module.nnunet import PlainConvUNet, DeepSupervisionTargetWrapper
from m3da_exp.torch.utils import nnunet_lr_schedule_fn
# from m3da_exp.train.base import train_and_save as train  # !!! re-defining train from core.config !!!


# # model
return_features_from = -1

# CT: [160, 192, 64]
# MRI: [224, 160, 48]
x_patch_size = y_patch_size = np.array([160, 160, 64])
batch_size = 2

pool_op_kernel_sizes = [[1, 1, 1], [2, 2, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]]  # (orig orientation)
conv_kernel_sizes = [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]
num_stages = len(conv_kernel_sizes)
base_num_features = 24  # 32
max_num_features = 320
features_per_stage = [min(base_num_features * 2 ** i, max_num_features) for i in range(num_stages)]
n_conv_per_stage_encoder = [2, 2, 2, 2, 2, 2]
n_conv_per_stage_decoder = [2, 2, 2, 2, 2]
norm_op = nn.BatchNorm3d
deep_supervision = True
architecture = PlainConvUNet(input_channels=n_chans_in, n_stages=num_stages, features_per_stage=features_per_stage,
                             conv_op=nn.Conv3d, kernel_sizes=conv_kernel_sizes, strides=pool_op_kernel_sizes,
                             n_conv_per_stage=n_conv_per_stage_encoder, num_classes=n_chans_out,
                             n_conv_per_stage_decoder=n_conv_per_stage_decoder, conv_bias=True, norm_op=norm_op,
                             norm_op_kwargs={'eps': 1e-5, 'affine': True}, dropout_op=None, dropout_op_kwargs=None,
                             nonlin=nn.LeakyReLU, nonlin_kwargs={'inplace': True}, deep_supervision=deep_supervision,
                             return_features_from=return_features_from)


# # loss
batch_dice = True
loss = DC_and_CE_loss({'batch_dice': batch_dice, 'smooth': 1e-5, 'do_bg': False, 'ddp': False}, {},
                      dice_class=MemoryEfficientSoftDiceLoss)
deep_supervision_scales = list(list(i) for i in 1 / np.cumprod(np.vstack(pool_op_kernel_sizes), axis=0))[:-1]
_supervision_weights = np.array([1 / (2 ** i) for i in range(len(deep_supervision_scales) - 1)] + [0])
# _supervision_weights[-1] = 0
supervision_weights = _supervision_weights / _supervision_weights.sum()
criterion_segm = DeepSupervisionTargetWrapper(DeepSupervisionWrapper(loss, supervision_weights),
                                              pool_op_kernel_sizes, labels_list, device)
criterion_ent = EntropyLoss()  # TODO: device?

# # optimizer
batches_per_epoch = 250
n_epochs = 400  # 300-400 are the best (val scores)
lr_init = 1e-2
lr = Switch(lr_init, {i: nnunet_lr_schedule_fn(i, lr_init, n_epochs=n_epochs) for i in range(n_epochs)})
weight_decay = 3e-5
clip_grad = 12

optimizer = torch.optim.SGD(architecture.parameters(), lr=lr_init, momentum=0.99, nesterov=True,
                            weight_decay=weight_decay)

# # train step
lambda_ent = 0.001
train_kwargs = dict(lr=lr, architecture=architecture, optimizer=optimizer, criterion_segm=criterion_segm,
                    criterion_ent=criterion_ent, lambda_ent=lambda_ent)


# # predict
pred_patch_size = x_patch_size
pred_patch_stride = pred_patch_size // 2


@add_extract_dims(n_add=pred_n_add_dims, n_extract=pred_n_extract_dims)
@patches_grid(pred_patch_size, pred_patch_stride, axis=SPATIAL_DIMS)
@divisible_shape(divisor=np.prod(pool_op_kernel_sizes, axis=0), padding_values=np.min, axis=SPATIAL_DIMS)
def predict(image):
    architecture.set_deep_supervision(False)
    pred = inference_step(image, architecture=architecture, activation=softmax, amp=amp)
    architecture.set_deep_supervision(True)
    return pred


@add_extract_dims(n_add=pred_n_add_dims, n_extract=pred_n_extract_dims)
@patches_grid(pred_patch_size, pred_patch_size, axis=SPATIAL_DIMS)
@divisible_shape(divisor=np.prod(pool_op_kernel_sizes, axis=0), padding_values=np.min, axis=SPATIAL_DIMS)
def val_predict(image):
    architecture.set_deep_supervision(False)
    pred = inference_step(image, architecture=architecture, activation=softmax, amp=amp)
    architecture.set_deep_supervision(True)
    return pred
